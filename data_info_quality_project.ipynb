{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling Image-based Social Sensing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "error",
     "timestamp": 1621959611277,
     "user": {
      "displayName": "Livio Biondo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi7vjbEUAp4Uy0CmWO5qYwzYBMhCwz3E9MqKDUmhA=s64",
      "userId": "01134029094434379991"
     },
     "user_tz": -120
    },
    "id": "gpYhiyR39lek",
    "outputId": "71a989f0-2186-4e64-8843-d21830d11243"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "missing = [\"-0\"]\n",
    "df = pd.read_csv('datasets/162_social_distancing_week_34_results_geoloc_ok.csv', na_values=missing)\n",
    "\n",
    "#assign a name to the first column that is unamed\n",
    "#df = df.rename(columns={df.columns[0]: 'line'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the columns names with their index in order to make it easier to reference them later\n",
    "cols = list(df.columns)\n",
    "length = len(df[cols[0]])\n",
    "\n",
    "i = 0\n",
    "while(i<len(cols)):\n",
    "    print(i,cols[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a dataset that contains data about all the countries in the world and select only the columns relative to the codes\n",
    "countries = pd.read_csv('datasets/ISO 3166-2  countries.csv')\n",
    "countries_codes = countries[[\"alpha-2\",\"alpha-3\"]]\n",
    "\n",
    "#read dataset that contains association between language code and country code\n",
    "languages = pd.read_csv('datasets/ietf-language-tags_csv.csv')\n",
    "languages = languages[[\"langType\",\"territory\"]]\n",
    "\n",
    "#integrate the two datasets\n",
    "languages = pd.merge(languages, countries_codes, how=\"inner\", left_on=['territory'], right_on=['alpha-2'])\n",
    "languages = languages.drop_duplicates().dropna()\n",
    "\n",
    "#for cleaness select only the columns relative to the post id, the alpha-3 country code and the language code\n",
    "df_reduced = df[[cols[22]] + [cols[27]] + [cols[32]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left join the data frame about the posts with the data frame with all the languages spoken in each country\n",
    "languages_merged = pd.merge(df_reduced, languages, how=\"left\",left_on=['info_country_code'], right_on=['alpha-3'])\n",
    "#select the rows that did not match\n",
    "languages_not_matching = languages_merged[languages_merged.isnull().any(axis=1)]\n",
    "languages_not_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute a ratio for language in the post not matching country's language\n",
    "df_length = len(df[cols[0]])\n",
    "not_matching_ratio = len(languages_not_matching[languages_not_matching.columns[0]]) / df_length\n",
    "print(\"Posts where the language doesn't match the country's languages: \"+str(100*not_matching_ratio)[0:5]+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compl is the compleateness measure for each column\n",
    "compl = 100 *(length - df.isnull().sum()) / length\n",
    "#to have the columns sorted\n",
    "#compl = compl.sort_values(ascending=False)\n",
    "\n",
    "#Keep only the rows that are incomplete\n",
    "compl = compl[compl < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=14)\n",
    "figure(figsize=(5, 7), dpi=200)\n",
    "ax = plt.axes()\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "\n",
    "if (compl.size == 0):\n",
    "    print(\"All columns are 100% complete\")\n",
    "else:\n",
    "    #print incomplete columns\n",
    "    print(\"Completeness:\")\n",
    "    for i, v in enumerate(compl):\n",
    "        plt.text(v + 1, i-0.25, str(v)[0:5] +\"%\", color='#0C764F',fontsize=10)\n",
    "print(compl.plot.barh())\n",
    "plt.tight_layout()\n",
    "plt.xlim([0, 145])\n",
    "#ax.set_facecolor(\"#FCF7EE\")\n",
    "plt.savefig(\"imgs/completeness_1.png\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create another dataframe where also Not Answered counts as null\n",
    "missing = [\"-0\",\"Not answered\"]\n",
    "df_2 = pd.read_csv('datasets/162_social_distancing_week_34_results_geoloc_ok.csv', na_values=missing)\n",
    "\n",
    "#compl_2 is the compleateness measure for each column considering Not Answerd as null\n",
    "compl_2 = 100 *(length - df_2.isnull().sum()) / length\n",
    "#compl_2 = compl_2.sort_values(ascending=False)\n",
    "\n",
    "#Keep only the rows that are incomplete\n",
    "compl_2 = compl_2[compl_2 < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Series to Dataframe\n",
    "compl_df = compl.to_frame().reset_index()\n",
    "compl_2_df = compl_2.to_frame().reset_index()\n",
    "\n",
    "#rename columns for joining\n",
    "compl_df = compl_df.rename(columns={compl_df.columns[0]: 'field'})\n",
    "compl_df = compl_df.rename(columns={compl_df.columns[1]: 'completeness_1'})\n",
    "compl_2_df = compl_2_df.rename(columns={compl_2_df.columns[0]: 'field'})\n",
    "compl_2_df = compl_2_df.rename(columns={compl_2_df.columns[1]: 'completeness_2'})\n",
    "\n",
    "compl_merged = pd.merge(compl_df, compl_2_df, how=\"inner\", left_on=['field'], right_on=['field'])\n",
    "#select only the columns where the completeness measure changes over the two datasets\n",
    "compl_diff = compl_merged.loc[compl_merged[\"completeness_1\"] != compl_merged[\"completeness_2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=14)\n",
    "figure(figsize=(5, 7), dpi=200)\n",
    "ax = plt.axes()\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "\n",
    "if (compl_2.size == 0):\n",
    "    print(\"All columns are 100% complete\")\n",
    "else:\n",
    "    print(\"Completeness:\")\n",
    "    for i, v in enumerate(compl_2):\n",
    "        #if an attribute has a different completeness measure when considering Not Answered as null print it in another color\n",
    "        if((compl_diff[compl_diff[\"field\"].str.contains(compl_2.axes[:][0][i])]).size):\n",
    "            plt.text(v + 1, i-0.25, str(v)[0:5] +\"%\", color='#990D00',fontsize=10)\n",
    "        else:\n",
    "            plt.text(v + 1, i-0.25, str(v)[0:5] +\"%\", color='#0C764F',fontsize=10)\n",
    "print(compl_2.plot.barh())\n",
    "plt.tight_layout()\n",
    "plt.xlim([0, 145])\n",
    "#ax.set_facecolor(\"#FCF7EE\")\n",
    "plt.savefig(\"imgs/completeness_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count how many posts for each country\n",
    "post_per_country = df.groupby(by=\"info_country_code\").count().reset_index()\n",
    "post_per_country = post_per_country.iloc[:,0:2]\n",
    "post_per_country = post_per_country.rename(columns={post_per_country.columns[1]: 'Posts'})\n",
    "post_per_country = post_per_country.sort_values(\"Posts\",ascending=False)\n",
    "post_per_country = post_per_country.rename(columns={post_per_country.columns[0]:\"Country Code\"})\n",
    "post_per_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a dataset with the population in each country\n",
    "world_pop = pd.read_csv('datasets/API_SP.POP.TOTL_DS2_en_csv_v2_2445260.csv')\n",
    "world_pop = world_pop[list(world_pop.columns[0:2]) + [world_pop.columns[-2]]]\n",
    "world_pop = world_pop.rename(columns={world_pop.columns[2]:\"Population\"})\n",
    "\n",
    "posts_population = post_per_country.set_index(\"Country Code\").join(world_pop.set_index(\"Country Code\"))\n",
    "columns_titles = [\"Country Name\", \"Posts\",\"Population\",\"Posts_Per_Person\"]\n",
    "posts_population=posts_population.reindex(columns=columns_titles)\n",
    "\n",
    "#compute the posts per person\n",
    "posts_population[\"Posts_Per_Person\"] = posts_population[\"Posts\"] / posts_population[\"Population\"]\n",
    "posts_population_ratio = posts_population.sort_values(\"Posts_Per_Person\",ascending=False)\n",
    "\n",
    "#remove countries with small population since with few data they easily become outliers\n",
    "posts_population_ratio = posts_population_ratio.loc[posts_population_ratio[\"Population\"] > 100000]\n",
    "\n",
    "posts_population_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# set the filepath and load\n",
    "fp = \"world_map/ne_110m_admin_0_countries.shp\"\n",
    "map_df = gpd.read_file(fp)\n",
    "post_per_country_map = map_df.set_index(\"ADM0_A3\").join(posts_population_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a variable that will call whatever column we want to visualise on the map\n",
    "variable = \"Posts\"\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = posts_population_ratio.Posts.max(), posts_population_ratio.Posts.min()\n",
    "\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(17, 6),dpi=200)\n",
    "\n",
    "post_per_country_map.plot(column=variable, cmap=\"BuGn\", linewidth=0.8, ax=ax, edgecolor=\"0.8\")\n",
    "\n",
    "# add a title\n",
    "ax.set_title(\"Posts per country\", fontdict={\"fontsize\": \"25\", \"fontweight\" : \"3\"})\n",
    "\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap=\"BuGn\", norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "# add the colorbar to the figure\n",
    "fig.colorbar( sm,shrink=0.9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"imgs/posts per country.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a variable that will call whatever column we want to visualise on the map\n",
    "variable = \"Posts_Per_Person\"\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = posts_population_ratio.Posts_Per_Person.max(), posts_population_ratio.Posts_Per_Person.min()\n",
    "\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig, ax = plt.subplots(1, figsize=(17, 6),dpi=200)\n",
    "\n",
    "post_per_country_map.plot(column=variable, cmap=\"BuGn\", linewidth=0.8, ax=ax, edgecolor=\"0.8\")\n",
    "\n",
    "# add a title\n",
    "ax.set_title(\"Posts per person\", fontdict={\"fontsize\": \"25\", \"fontweight\" : \"3\"})\n",
    "\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap=\"BuGn\", norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "#add colorbar\n",
    "fig.colorbar( sm,shrink=0.9)\n",
    "#fig.set_facecolor(\"#FCF7EE\")\n",
    "#ax.set_facecolor(\"#FCF7EE\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"imgs/posts per person.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the cells with data to 1, the ones with no data to 0\n",
    "ds = df_2[cols[4:19]]\n",
    "invert_one_zero = {0: 1,1: 0}\n",
    "\n",
    "ds = ds.isnull().astype(int)\n",
    "ds =ds.replace(invert_one_zero)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the correlation matrix\n",
    "ds.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#print the correlation matrix\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = ds.corr(method ='pearson')\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)\n",
    "plt.savefig(\"imgs/correlation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if to each country is associated only one country code\n",
    "a = df[cols[26:29]]\n",
    "#a.groupby(['info_country']).info_country_code.nunique()\n",
    "tab = a.groupby(['info_country_or_territory']).info_country_code.nunique().reset_index()\n",
    "tab[(tab[\"info_country_code\"] > 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken Links\n",
    "\n",
    "Since often during the computation the program hangs, it's necessasry to restart it. In order to not restart from the beginning the progress is saved in a file *broken_links.pkl*. To start from the beginnnig it is necessary to delete the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import validators\n",
    "from IPython.display import display, clear_output\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "links = df[\"info_media_url\"]\n",
    "\n",
    "try:\n",
    "    with open('broken_links.pkl', \"rb\") as f:\n",
    "        line, broken_links = pickle.load(f)\n",
    "except:\n",
    "    broken_links = 0\n",
    "    line = 0\n",
    "#in case I want to print/save the broken links    \n",
    "broken_links_list = []\n",
    "\n",
    "for link in links[line:]:\n",
    "    #remove previous prints\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(\"Reading line \"+str(line + 1)+ \" of \"+str(length)+\"\\nBroken links:\"+ str(broken_links))\n",
    "    \n",
    "    #check is the url is valid\n",
    "    if not validators.url(link):\n",
    "        broken_links +=1\n",
    "        line+=1\n",
    "        with open('broken_links.pkl', \"wb\") as f:\n",
    "            pickle.dump([line, broken_links], f)\n",
    "        continue\n",
    "    \n",
    "    #check if the pic is still available    \n",
    "    response = requests.get(link)\n",
    "    if(response.status_code > 400):\n",
    "        broken_links +=1\n",
    "        broken_links_list.append(link)\n",
    "        \n",
    "    \n",
    "    line+=1\n",
    "\n",
    "    with open('broken_links.pkl', \"wb\") as f:\n",
    "         pickle.dump([line, broken_links], f)\n",
    "    #Twitter doesn't allow more than 1 request per second\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "if(line == length):\n",
    "    print(\"Broken links: \"+ str(broken_links)+ \" of \"+str(length)+\" lines\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "#pandas_profiling.ProfileReport(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBDbJKmK0CykL+r8H+7Q+p",
   "collapsed_sections": [],
   "name": "Data Info Quality Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
